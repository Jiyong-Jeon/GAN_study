{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxlvhnWGREktq5UVx+l6fJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiyong-Jeon/GAN_study/blob/main/GAN_In_Action/FashionMnist_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Auto-Encoder vs Variational Auto-Encoder\n",
        "  - latent vector 분포 차이 확인\n",
        "2. Convalution Variational Auto-Encoder\n",
        "3. 성능 향상 해보기"
      ],
      "metadata": {
        "id": "hSZ5_YAGSY_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2BunZ5KSS42"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "  print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
        "  pass\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQebQnKsS3aY",
        "outputId": "0d5eabbc-95cc-41e9-ce50-7abea7789036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cuda: Tesla T4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fashion Mnist Dataset"
      ],
      "metadata": {
        "id": "yXdW9diqZBUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_list = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankel boot']\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "                                # transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device=device))\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyZ7J-lS5d_",
        "outputId": "ce46d3ea-e987-409a-c35b-7075b8d97f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 9851678.81it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 175569.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3233297.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 17177626.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(image, label):\n",
        "    plt.title(\"label = \"+ class_list[label])\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(image, interpolation='none', cmap='Blues')"
      ],
      "metadata": {
        "id": "dTNE3jVrS57_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(trainloader))\n",
        "print(image.shape, label.shape)\n",
        "imshow(image[0,:], label[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Q26rs2YCY8f4",
        "outputId": "5d2cd044-1e06-4497-8dc4-bffeb3d4b300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Auto-Encoder vs Variational Auto-Encoder"
      ],
      "metadata": {
        "id": "xC_wX_DzZtmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주요 공통 변수\n",
        "original_dim = 784    # MNIST 이미지의 높이 x 너비\n",
        "latent_dim = 2\n",
        "hidden_dim_1 = 256\n",
        "hidden_dim_2 = 64\n",
        "epochs = 50           # 애폭 수"
      ],
      "metadata": {
        "id": "GvB-4CvGZ2fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE"
      ],
      "metadata": {
        "id": "XDJsGbVyZyGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE model"
      ],
      "metadata": {
        "id": "MM4kM7k8-VaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(original_dim, hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mean_dc = nn.Linear(hidden_dim_2, latent_dim)\n",
        "        self.logvar_dc = nn.Linear(hidden_dim_2, latent_dim)\n",
        "\n",
        "    def sampling(self, z_mean, z_logvar):\n",
        "        z_std = torch.exp(z_logvar / 2)\n",
        "        eps = torch.randn_like(z_std)\n",
        "        return z_mean + eps*z_std\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_mean = self.mean_dc(x)\n",
        "        z_logvar = self.logvar_dc(x)\n",
        "        z = self.sampling(z_mean, z_logvar)\n",
        "        return z_mean, z_logvar, z\n"
      ],
      "metadata": {
        "id": "ABK_HtHyY9xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_2, hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_1, original_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.decoder(z)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "EM9OpqeMe7t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE Train"
      ],
      "metadata": {
        "id": "BXxjEiXYhdZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function\n",
        "def vae_loss_function(recon_x, x, z_mean, z_logvar):\n",
        "    BCE_loss = nn.functional.binary_cross_entropy(recon_x, x.view(-1, original_dim) , reduction=\"sum\" ) # default = mean\n",
        "    KL_loss = -0.5 * torch.sum(1 + z_logvar - torch.exp(z_logvar) - z_mean.pow(2))\n",
        "    return BCE_loss + KL_loss"
      ],
      "metadata": {
        "id": "jsOcXlK5gfbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_train(epochs, encoder, decoder, trainloader, optimizer):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    train_loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "        temp_train_loss = 0\n",
        "        for image_data, _ in trainloader:\n",
        "            image_data = image_data.to(device)\n",
        "            z_mean, z_logvar, z = encoder(image_data.view(-1, original_dim))\n",
        "            recon_x = decoder(z)\n",
        "\n",
        "            loss = vae_loss_function(recon_x, image_data, z_mean, z_logvar)\n",
        "            temp_train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = temp_train_loss / len(trainloader.dataset)\n",
        "        train_loss_list.append(train_loss)\n",
        "        print(\">>>> epoch: {}/{} || loss: {:.4f}\".format(epoch, epochs, train_loss))\n",
        "\n",
        "    return train_loss_list"
      ],
      "metadata": {
        "id": "dAxLU54ljLPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_encoder = VAE_Encoder().to(device)\n",
        "vae_decoder = VAE_Decoder().to(device)\n",
        "optimizer = torch.optim.Adam(list(vae_encoder.parameters()) + list(vae_decoder.parameters()), lr=0.0001)"
      ],
      "metadata": {
        "id": "nKuwE38lno0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "vae_train_loss= vae_train(epochs, vae_encoder, vae_decoder, trainloader, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKZG2EfWokMD",
        "outputId": "8120d3ee-d90d-465a-bca9-18a1def136eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> epoch: 0/50 || loss: 347.4747\n",
            ">>>> epoch: 1/50 || loss: 284.5077\n",
            ">>>> epoch: 2/50 || loss: 279.6204\n",
            ">>>> epoch: 3/50 || loss: 276.6316\n",
            ">>>> epoch: 4/50 || loss: 274.1478\n",
            ">>>> epoch: 5/50 || loss: 272.1599\n",
            ">>>> epoch: 6/50 || loss: 270.6947\n",
            ">>>> epoch: 7/50 || loss: 269.6532\n",
            ">>>> epoch: 8/50 || loss: 268.8703\n",
            ">>>> epoch: 9/50 || loss: 268.1663\n",
            ">>>> epoch: 10/50 || loss: 267.5649\n",
            ">>>> epoch: 11/50 || loss: 266.9820\n",
            ">>>> epoch: 12/50 || loss: 266.4746\n",
            ">>>> epoch: 13/50 || loss: 265.9820\n",
            ">>>> epoch: 14/50 || loss: 265.4665\n",
            ">>>> epoch: 15/50 || loss: 265.0598\n",
            ">>>> epoch: 16/50 || loss: 264.6629\n",
            ">>>> epoch: 17/50 || loss: 264.3104\n",
            ">>>> epoch: 18/50 || loss: 263.9777\n",
            ">>>> epoch: 19/50 || loss: 263.6903\n",
            ">>>> epoch: 20/50 || loss: 263.4334\n",
            ">>>> epoch: 21/50 || loss: 263.1973\n",
            ">>>> epoch: 22/50 || loss: 262.9505\n",
            ">>>> epoch: 23/50 || loss: 262.7600\n",
            ">>>> epoch: 24/50 || loss: 262.5799\n",
            ">>>> epoch: 25/50 || loss: 262.4309\n",
            ">>>> epoch: 26/50 || loss: 262.2301\n",
            ">>>> epoch: 27/50 || loss: 262.0780\n",
            ">>>> epoch: 28/50 || loss: 261.9446\n",
            ">>>> epoch: 29/50 || loss: 261.7900\n",
            ">>>> epoch: 30/50 || loss: 261.6203\n",
            ">>>> epoch: 31/50 || loss: 261.5281\n",
            ">>>> epoch: 32/50 || loss: 261.3866\n",
            ">>>> epoch: 33/50 || loss: 261.2469\n",
            ">>>> epoch: 34/50 || loss: 261.1663\n",
            ">>>> epoch: 35/50 || loss: 261.0129\n",
            ">>>> epoch: 36/50 || loss: 260.9201\n",
            ">>>> epoch: 37/50 || loss: 260.8123\n",
            ">>>> epoch: 38/50 || loss: 260.7097\n",
            ">>>> epoch: 39/50 || loss: 260.5941\n",
            ">>>> epoch: 40/50 || loss: 260.4841\n",
            ">>>> epoch: 41/50 || loss: 260.3951\n",
            ">>>> epoch: 42/50 || loss: 260.2994\n",
            ">>>> epoch: 43/50 || loss: 260.2038\n",
            ">>>> epoch: 44/50 || loss: 260.1040\n",
            ">>>> epoch: 45/50 || loss: 260.0321\n",
            ">>>> epoch: 46/50 || loss: 259.9197\n",
            ">>>> epoch: 47/50 || loss: 259.8359\n",
            ">>>> epoch: 48/50 || loss: 259.7509\n",
            ">>>> epoch: 49/50 || loss: 259.6517\n",
            "CPU times: user 7min 8s, sys: 2.89 s, total: 7min 11s\n",
            "Wall time: 7min 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE 결과"
      ],
      "metadata": {
        "id": "XIUjBTjkz66i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_test(encoder, decoder, testloader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_data, label = next(iter(trainloader))\n",
        "        image_data = image_data.to(device)\n",
        "        z_mean, z_logvar, z = encoder(image_data.view(-1, original_dim))\n",
        "        recon_x = decoder(z)\n",
        "\n",
        "        loss = loss_function(recon_x, image_data, z_mean, z_logvar)\n",
        "        print(\">>>> test loss: {:.4f}\".format(loss / len(image_data)))\n",
        "\n",
        "        plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
        "\n",
        "        image_data = image_data.cpu()\n",
        "        origin_grid = make_grid(image_data, nrow=int(batch_size**0.5), normalize=True).permute(1,2,0).numpy()\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"original image\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(origin_grid)\n",
        "\n",
        "        sample_images = recon_x.view(-1, 1, 28, 28).cpu()\n",
        "        recon_grid = make_grid(sample_images, nrow=int(batch_size**0.5), normalize=True).permute(1,2,0).numpy()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"reconstruct image\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(recon_grid)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "dk12Hu2Boxiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_test(vae_encoder, vae_decoder, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "ung9rPvE18CD",
        "outputId": "48c44bbe-a8e9-4848-f596-04520008b307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2f036e175b78>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-650f6a10904e>\u001b[0m in \u001b[0;36mvae_test\u001b[0;34m(encoder, decoder, testloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>> test loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_function' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_scatter(encoder, decoder, testloader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    with torch.no_grad():\n",
        "        for image_data, label in testloader:\n",
        "            image_data = image_data.to(device)\n",
        "            z_mean, z_logvar, z = encoder(image_data.view(-1, original_dim))\n",
        "            z_mean = z_mean.cpu().numpy()\n",
        "            # z_logvar = z_logvar.cpu().numpy()\n",
        "            # z = z.cpu().numpy()\n",
        "\n",
        "            plt.scatter(z_mean[:,0], z_mean[:,1], c=label.cpu(), cmap='viridis')\n",
        "            # plt.scatter(z_logvar[:,0], z_logvar[:,1], c=label.cpu(), cmap='viridis')\n",
        "            # plt.scatter(z[:,0], z[:,1], c=label.cpu(), cmap='viridis')\n",
        "\n",
        "\n",
        "    plt.colorbar()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "D87WJtlG3tBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_scatter(vae_encoder, vae_decoder, testloader)"
      ],
      "metadata": {
        "id": "Gs9-REFH4OZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AE"
      ],
      "metadata": {
        "id": "NlDkD6JB-cjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AE model"
      ],
      "metadata": {
        "id": "JumPNugL-hX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AE_Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(original_dim, hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_2, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "jnHAzo_Y-eA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AE_Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_2, hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_1, original_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.decoder(z)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Q6sYC-lp-vr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AE train"
      ],
      "metadata": {
        "id": "Hlp8dAmQ-8Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function\n",
        "def ae_loss_function(recon_x, x):\n",
        "    BCE_loss = nn.functional.binary_cross_entropy(recon_x, x.view(-1, original_dim) , reduction=\"sum\" ) # default = mean\n",
        "    return BCE_loss"
      ],
      "metadata": {
        "id": "A1dcMS3M-yMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_train(epochs, encoder, decoder, trainloader, optimizer):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    train_loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "        temp_train_loss = 0\n",
        "        for image_data, _ in trainloader:\n",
        "            image_data = image_data.to(device)\n",
        "            z = encoder(image_data.view(-1, original_dim))\n",
        "            recon_x = decoder(z)\n",
        "\n",
        "            loss = ae_loss_function(recon_x, image_data)\n",
        "            temp_train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = temp_train_loss / len(trainloader.dataset)\n",
        "        train_loss_list.append(train_loss)\n",
        "        print(\">>>> epoch: {}/{} || loss: {:.4f}\".format(epoch, epochs, train_loss))\n",
        "\n",
        "    return train_loss_list"
      ],
      "metadata": {
        "id": "xdbot6Lc_Cvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_encoder = AE_Encoder().to(device)\n",
        "ae_decoder = AE_Decoder().to(device)\n",
        "optimizer = torch.optim.Adam(list(ae_encoder.parameters()) + list(ae_decoder.parameters()), lr=0.0001)"
      ],
      "metadata": {
        "id": "hnPDqTMI_JoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ae_train_loss= ae_train(epochs, ae_encoder, ae_decoder, trainloader, optimizer)"
      ],
      "metadata": {
        "id": "KAX2YxaX_Ow-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AE 결과"
      ],
      "metadata": {
        "id": "6KY48BeO__nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_test(encoder, decoder, testloader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_data, label = next(iter(trainloader))\n",
        "        image_data = image_data.to(device)\n",
        "        z = encoder(image_data.view(-1, original_dim))\n",
        "        recon_x = decoder(z)\n",
        "\n",
        "        loss = ae_loss_function(recon_x, image_data)\n",
        "        print(\">>>> test loss: {:.4f}\".format(loss / len(image_data)))\n",
        "\n",
        "        plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
        "\n",
        "        image_data = image_data.cpu()\n",
        "        origin_grid = make_grid(image_data, nrow=int(batch_size**0.5), normalize=True).permute(1,2,0).numpy()\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"original image\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(origin_grid)\n",
        "\n",
        "        sample_images = recon_x.view(-1, 1, 28, 28).cpu()\n",
        "        recon_grid = make_grid(sample_images, nrow=int(batch_size**0.5), normalize=True).permute(1,2,0).numpy()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"reconstruct image\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(recon_grid)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "q5L2XfmY_5-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_test(ae_encoder, ae_decoder, testloader)"
      ],
      "metadata": {
        "id": "i-NJLIX9AKjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_scatter(encoder, decoder, testloader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    with torch.no_grad():\n",
        "        for image_data, label in testloader:\n",
        "            image_data = image_data.to(device)\n",
        "            z = encoder(image_data.view(-1, original_dim))\n",
        "            z = z.cpu().numpy()\n",
        "            # z_logvar = z_logvar.cpu().numpy()\n",
        "            # z = z.cpu().numpy()\n",
        "\n",
        "            plt.scatter(z[:,0], z[:,1], c=label.cpu(), cmap='viridis')\n",
        "            # plt.scatter(z_logvar[:,0], z_logvar[:,1], c=label.cpu(), cmap='viridis')\n",
        "            # plt.scatter(z[:,0], z[:,1], c=label.cpu(), cmap='viridis')\n",
        "\n",
        "\n",
        "    plt.colorbar()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cJtJqSLcAQQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_scatter(vae_encoder, vae_decoder, testloader)"
      ],
      "metadata": {
        "id": "VuLS-T2gAZ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_scatter(ae_encoder, ae_decoder, testloader)"
      ],
      "metadata": {
        "id": "1Vd1kY8kAaPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmTT-Le9AdD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}